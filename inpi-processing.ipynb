{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7de265f",
   "metadata": {},
   "source": [
    "Do in shell :\n",
    "\n",
    "`mkdir tmpzip && mkdir tmp && mkdir data && cd data`\n",
    "\n",
    "`wget -r --no-parent https://data.cquest.org/inpi_rncs/imr/`\n",
    "\n",
    "`mkdir data.cquest.org/inpi_rncs/imr/flux`\n",
    "\n",
    "`mv data.cquest.org/inpi_rncs/imr/20* data.cquest.org/inpi_rncs/imr/flux/`\n",
    "\n",
    "2017:\n",
    "`for f in *.zip; do mkdir ${f:0:4} && mkdir ${f:0:4}/${f:5:2} && unzip $f -d ${f:0:4}/${f:5:2}/ && rm *.zip && rm *.html; done`\n",
    "\n",
    "2018 ok\n",
    "\n",
    "2019, 2020, 2021, 2022\n",
    "`for f in */*/*.zip; do mkdir ${f:0:2}/${f:3:2}/${f:6:4} && mkdir ${f:0:2}/${f:3:2}/${f:6:4}/${f:11:2} && unzip $f -d ${f:0:2}/${f:3:2}/${f:6:4}/${f:11:2}/; done`\n",
    "`rm */*/*.html && rm */*/*.zip && rm */*/*.md5`\n",
    "\n",
    "On ordonne le tout pour que toutes les données soient dans data/TYPE_DONNEES/ANNEE/MOIS/JOUR/GREFFECODE/GREFFECODE2/DONNEESCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import shutil\n",
    "import zipfile\n",
    "import glob\n",
    "import unidecode\n",
    "import os\n",
    "import py7zr\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from os.path import exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27218ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdate = date(2017,5,4)   # start date\n",
    "edate = date(2022,3,17)   # end date\n",
    "list_dates = [x.strftime(\"%Y-%m-%d\") for x in pd.date_range(sdate,edate-timedelta(days=1),freq='d').to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_data = '/Volumes/OnlyData/inpi/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ef664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape de processing des données en amont du lancement de l'enrichissement de la bdd sqlite\n",
    "def concatFilesRep(type_file, name_concat, pattern):\n",
    "    pathdate = vol_data + type_file + \"/\" + ld.split('-')[0] + '/' + ld.split('-')[1] + '/' + ld.split('-')[2] + '/'\n",
    "    consofile = vol_data + 'synthese/' + ld + name_concat + '.csv'\n",
    "    # get list files stock\n",
    "    list_files = glob.glob(pathdate + pattern)\n",
    "    list_files.sort()\n",
    "    if len(list_files) > 0:\n",
    "        \n",
    "        with open(consofile,\"wb\") as fout:\n",
    "            # first file:\n",
    "            with open(list_files[0], \"rb\") as f:\n",
    "                fout.write(f.read())\n",
    "            # now the rest:    \n",
    "            for ls in list_files[1:]:\n",
    "                with open(ls, \"rb\") as f:\n",
    "                    try:\n",
    "                        next(f) # skip the header\n",
    "                    except:\n",
    "                        pass\n",
    "                    fout.write(f.read())\n",
    "        \n",
    "        \n",
    "        #os.system('rm ' + consofile + ' && ulimit -n 10240 && head -n1 ' + list_files[0] + ' > ' + consofile)\n",
    "        #for ls in list_files:\n",
    "        #    os.system('tail -n+2 -q ' + ls + ' >> ' + consofile)\n",
    "        \n",
    "        print(name_concat + \" ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f327d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ld in list_dates[895:]:\n",
    "    print(ld)\n",
    "    concatFilesRep(\"stock\", \"_stock_rep\", \"*/*/*_5_rep.csv\")\n",
    "    concatFilesRep(\"flux-tc\", \"_flux_rep\", \"*/*/*_5_rep.csv\")\n",
    "    concatFilesRep(\"flux-tc\", \"_flux_rep_nouveau_modifie\", \"*/*/*6_rep_nouveau_modifie_EVT.csv\")\n",
    "    concatFilesRep(\"flux-tc\", \"_flux_rep_partant\", \"*/*/*7_rep_partant_EVT.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm inpi.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('inpi.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6611879",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bbdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''CREATE TABLE IF NOT EXISTS rep_pp\n",
    "              (siren TEXT, rep_noms TEXT, rep_prenoms TEXT, rep_qualite)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''CREATE TABLE IF NOT EXISTS rep_pm\n",
    "              (siren TEXT, rep_denomination TEXT, rep_qualite)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6915fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''CREATE INDEX idx_siren_pp ON rep_pp (siren);''')\n",
    "cursor.execute('''CREATE INDEX idx_siren_pm ON rep_pm (siren);''')\n",
    "cursor.execute('''CREATE INDEX idx_noms_pp ON rep_pp (rep_noms);''')\n",
    "cursor.execute('''CREATE INDEX idx_denomination_pm ON rep_pm (rep_denomination);''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformizeDf(df):\n",
    "    for c in df.columns:\n",
    "        df = df.rename(columns={c: unidecode.unidecode(\n",
    "                                c.lower() \\\n",
    "                                .replace(' ', '') \\\n",
    "                                .replace('.','') \\\n",
    "                                .replace('_','') \\\n",
    "                                .replace('\"','') \\\n",
    "                                .replace(\"'\",'')\n",
    "                    )})\n",
    "\n",
    "    # Do something\n",
    "    df['siren'] = df.fillna('')['siren']\n",
    "    df['rep_noms'] = df.fillna('')['nompatronymique']\n",
    "    df.loc[df[\"nomusage\"].notna(), \"rep_noms\"] = df[\"nompatronymique\"] + ' ' + df['nomusage']\n",
    "    df['rep_noms'] = df['rep_noms'].apply(lambda x: str(x).replace(',','').lower())\n",
    "    df['rep_prenoms'] = df.fillna('')['prenoms']\n",
    "    df['rep_prenoms'] = df['rep_prenoms'].apply(lambda x: str(x).replace(',','').lower())\n",
    "    df['rep_denomination'] = df.fillna('')['denomination']\n",
    "    df['rep_qualite'] = df.fillna('')['qualite']\n",
    "    dfpp = df[df['type'].str.contains('Physique')][['siren','rep_noms','rep_prenoms','rep_qualite']]\n",
    "    dfpm = df[df['type'].str.contains('Morale')][['siren','rep_denomination','rep_qualite']]\n",
    "\n",
    "    return dfpp, dfpm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd62e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On s'occupe du stock initial (car vraiment gros et pas adapté à la méthode ci-dessous)\n",
    "list_files = glob.glob(vol_data + 'stock/2017/05/04/*/*/*5_rep.csv')\n",
    "list_files.sort()\n",
    "for lf in list_files:\n",
    "    print(lf)\n",
    "    df = pd.read_csv(lf,sep=\";\",dtype=str)\n",
    "    dfpp, dfpm = uniformizeDf(df)\n",
    "    dfpp.set_index('siren').to_sql('rep_pp', con=connection, if_exists='append')\n",
    "    dfpm.set_index('siren').to_sql('rep_pm', con=connection, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d501e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = 'inpi.db'\n",
    "target = 'inpi-stock-initial.db'\n",
    "shutil.copyfile(original, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates[698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a41f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toutes les dates après la date du stock initial\n",
    "for ld in list_dates[1:]:\n",
    "    print(ld)\n",
    "    if ld.split('-')[2] == '01':\n",
    "        original = 'inpi.db'\n",
    "        target = 'inpi-'+ld+'.db'\n",
    "        shutil.copyfile(original, target)\n",
    "    \n",
    "    consofile = vol_data + 'synthese/' + ld + '_stock_rep.csv'\n",
    "    # Manage stocks\n",
    "    if(os.path.exists(consofile)):\n",
    "        df = pd.read_csv(consofile,sep=\";\",dtype=str,on_bad_lines='warn')\n",
    "        dfpp, dfpm = uniformizeDf(df)\n",
    "        print('loaded and uniformize')\n",
    "        # Delete old stock\n",
    "        if(ld != '2017-05-04'):\n",
    "            print('Old stock')\n",
    "            result = 0\n",
    "            for index,row in dfpp.iterrows():\n",
    "                del_query = \"\"\"DELETE from rep_pp where siren = ?\"\"\"\n",
    "                cursor.execute(del_query, (row['siren'],))\n",
    "                result = result + cursor.rowcount\n",
    "            connection.commit()\n",
    "            for index,row in dfpp.iterrows():\n",
    "                del_query = \"\"\"DELETE from rep_pm where siren = ?\"\"\"\n",
    "                cursor.execute(del_query, (row['siren'],))\n",
    "                result = result + cursor.rowcount\n",
    "            connection.commit()\n",
    "            print('Old Stock deleted : ' + str(result) + ' deleted records')\n",
    "        # Add new stock\n",
    "        dfpp.set_index('siren').to_sql('rep_pp', con=connection, if_exists='append')\n",
    "        dfpm.set_index('siren').to_sql('rep_pm', con=connection, if_exists='append')\n",
    "        print('Stock processed : ' + str(dfpp.shape[0]+dfpm.shape[0]) + 'added records')\n",
    "\n",
    "    \n",
    "    consofile = vol_data + 'synthese/' + ld + '_flux_rep.csv'\n",
    "    if(os.path.exists(consofile)):\n",
    "        df = pd.read_csv(consofile,sep=\";\",dtype=str,on_bad_lines='warn')\n",
    "        dfpp, dfpm = uniformizeDf(df)\n",
    "        dfpp.set_index('siren').to_sql('rep_pp', con=connection, if_exists='append')\n",
    "        dfpm.set_index('siren').to_sql('rep_pm', con=connection, if_exists='append')\n",
    "        print('Flux rep processed : ' + str(dfpp.shape[0]+dfpm.shape[0]) + 'added records')\n",
    "\n",
    "    consofile = vol_data + 'synthese/' + ld + '_flux_rep_nouveau_modifie.csv'\n",
    "    if(os.path.exists(consofile)):\n",
    "        df = pd.read_csv(consofile,sep=\";\",dtype=str,on_bad_lines='warn')\n",
    "        dfpp, dfpm = uniformizeDf(df)\n",
    "        dfpp.set_index('siren').to_sql('rep_pp', con=connection, if_exists='append')\n",
    "        dfpm.set_index('siren').to_sql('rep_pm', con=connection, if_exists='append')\n",
    "        print('Flux rep modified new processed : ' + str(dfpp.shape[0]+dfpm.shape[0]) + 'added records')\n",
    "\n",
    "    \n",
    "    consofile = vol_data + 'synthese/' + ld + '_flux_rep_partant.csv'\n",
    "    if(os.path.exists(consofile)):\n",
    "        df = pd.read_csv(consofile,sep=\";\",dtype=str,on_bad_lines='warn')\n",
    "        dfpp, dfpm = uniformizeDf(df)\n",
    "        # Delete each rep\n",
    "        result = 0\n",
    "        for index,row in dfpp.iterrows():\n",
    "            del_query = \"\"\"DELETE from rep_pp where siren = ? AND rep_noms = ? AND rep_prenoms = ? AND rep_qualite = ?\"\"\"\n",
    "            cursor.execute(del_query, (row['siren'], row['rep_noms'], row['rep_prenoms'], row['rep_qualite']))\n",
    "            result = result + cursor.rowcount\n",
    "        connection.commit()\n",
    "        for index,row in dfpm.iterrows():\n",
    "            del_query = \"\"\"DELETE from rep_pm where siren = ? AND rep_denomination = ? AND rep_qualite = ?\"\"\"\n",
    "            cursor.execute(del_query, (row['siren'], row['rep_denomination'], row['rep_qualite']))\n",
    "            result = result + cursor.rowcount\n",
    "        connection.commit()\n",
    "        print('Flux rep partant processed : ' + str(result) + ' deleted records')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de37956",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('inpi.db', isolation_level=None,\n",
    "                       detect_types=sqlite3.PARSE_COLNAMES)\n",
    "print('connected')\n",
    "\n",
    "db = pd.read_sql_query(\"SELECT * FROM rep_pm\", conn)\n",
    "print('read rep_pm ok')\n",
    "db = db.drop_duplicates(keep=\"first\")\n",
    "print('deduplicates ok')\n",
    "dbpm = db.groupby(['siren','rep_denomination'],as_index=False)['rep_qualite'].agg(lambda col: ', '.join(col))\n",
    "print('groupby ok')\n",
    "dbpm[dbpm['siren'] != ''].to_csv('rep_pm.csv',index=False)\n",
    "print('csv saved ok')\n",
    "\n",
    "db = pd.read_sql_query(\"SELECT * FROM rep_pp\", conn)\n",
    "print('read rep_pp ok')\n",
    "db = db.drop_duplicates(keep=\"first\")\n",
    "print('deduplicates ok')\n",
    "dbpp = db.groupby(['siren','rep_noms','rep_prenoms'],as_index=False)['rep_qualite'].agg(lambda col: ', '.join(col))\n",
    "print('groupby ok')\n",
    "dbpp[dbpp['siren'] != ''].to_csv('rep_pp.csv',index=False)\n",
    "print('csv saved ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
